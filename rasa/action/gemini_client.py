"""
Gemini API å®¢æˆ¶ç«¯æ¨¡çµ„
å°è£ Google Gemini API èª¿ç”¨ï¼Œæä¾›å®‰å…¨çš„ API key ç®¡ç†å’ŒéŒ¯èª¤è™•ç†
å„ªåŒ–ç‰ˆæœ¬ï¼šåŒ…å«ç·©å­˜ã€æç¤ºè©å„ªåŒ–ã€éŸ¿æ‡‰è³ªé‡æå‡ç­‰åŠŸèƒ½
"""

import os
import logging
import time
import hashlib
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
from collections import OrderedDict
import google.generativeai as genai

logger = logging.getLogger(__name__)


class ResponseCache:
    """
    éŸ¿æ‡‰ç·©å­˜é¡
    ç”¨æ–¼ç·©å­˜å¸¸è¦‹å•é¡Œçš„å›æ‡‰ï¼Œæ¸›å°‘ API èª¿ç”¨
    """
    
    def __init__(self, max_size: int = 100, ttl: int = 3600):
        """
        åˆå§‹åŒ–ç·©å­˜
        
        Args:
            max_size: æœ€å¤§ç·©å­˜æ¢ç›®æ•¸
            ttl: ç·©å­˜éæœŸæ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.cache: OrderedDict = OrderedDict()
        self.timestamps: Dict[str, datetime] = {}
        self.max_size = max_size
        self.ttl = ttl
    
    def _generate_key(self, message: str, language: str) -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        key_string = f"{language}:{message.strip().lower()}"
        return hashlib.md5(key_string.encode('utf-8')).hexdigest()
    
    def get(self, message: str, language: str) -> Optional[str]:
        """
        ç²å–ç·©å­˜å›æ‡‰
        
        Args:
            message: ç”¨æˆ¶è¨Šæ¯
            language: èªè¨€ä»£ç¢¼
            
        Returns:
            ç·©å­˜å›æ‡‰ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–å·²éæœŸå‰‡è¿”å› None
        """
        key = self._generate_key(message, language)
        
        if key not in self.cache:
            return None
        
        # æª¢æŸ¥æ˜¯å¦éæœŸ
        if key in self.timestamps:
            if datetime.now() - self.timestamps[key] > timedelta(seconds=self.ttl):
                del self.cache[key]
                del self.timestamps[key]
                return None
        
        # æ›´æ–°è¨ªå•é †åºï¼ˆLRUï¼‰
        response = self.cache.pop(key)
        self.cache[key] = response
        return response
    
    def set(self, message: str, language: str, response: str) -> None:
        """
        è¨­ç½®ç·©å­˜å›æ‡‰
        
        Args:
            message: ç”¨æˆ¶è¨Šæ¯
            language: èªè¨€ä»£ç¢¼
            response: API å›æ‡‰
        """
        key = self._generate_key(message, language)
        
        # å¦‚æœç·©å­˜å·²æ»¿ï¼Œåˆªé™¤æœ€èˆŠçš„æ¢ç›®
        if len(self.cache) >= self.max_size:
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            if oldest_key in self.timestamps:
                del self.timestamps[oldest_key]
        
        self.cache[key] = response
        self.timestamps[key] = datetime.now()
    
    def clear(self) -> None:
        """æ¸…ç©ºç·©å­˜"""
        self.cache.clear()
        self.timestamps.clear()


class GeminiClient:
    """
    Gemini API å®¢æˆ¶ç«¯ï¼ˆå„ªåŒ–ç‰ˆï¼‰
    è² è²¬èˆ‡ Google Gemini API é€šä¿¡ï¼ŒåŒ…å«ç·©å­˜ã€æç¤ºè©å„ªåŒ–ç­‰åŠŸèƒ½
    """
    
    def __init__(self):
        """åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯"""
        self.api_key = os.getenv('GEMINI_API_KEY')
        self.model_name = os.getenv('GEMINI_MODEL', 'gemini-2.0-flash-exp')
        self.is_configured = False
        
        # åˆå§‹åŒ–ç·©å­˜
        cache_size = int(os.getenv('GEMINI_CACHE_SIZE', '100'))
        cache_ttl = int(os.getenv('GEMINI_CACHE_TTL', '3600'))
        self.cache = ResponseCache(max_size=cache_size, ttl=cache_ttl)
        
        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'api_errors': 0,
            'successful_responses': 0
        }
        
        if self.api_key:
            try:
                genai.configure(api_key=self.api_key)
                self.is_configured = True
                logger.info(f"Gemini API å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆæ¨¡å‹: {self.model_name}ï¼‰")
            except Exception as e:
                logger.error(f"Gemini API é…ç½®å¤±æ•—: {str(e)}")
                self.is_configured = False
        else:
            logger.warning("GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­ç½®ï¼ŒGemini åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
    
    def is_available(self) -> bool:
        """
        æª¢æŸ¥ Gemini API æ˜¯å¦å¯ç”¨
        
        Returns:
            bool: å¦‚æœ API key å·²è¨­ç½®ä¸”é…ç½®æˆåŠŸå‰‡è¿”å› True
        """
        return self.is_configured and self.api_key is not None
    
    def generate_response(
        self,
        user_message: str,
        conversation_context: Optional[list] = None,
        language: str = 'zh',
        max_retries: int = 2,
        use_cache: bool = True
    ) -> Optional[str]:
        """
        ç”Ÿæˆå›æ‡‰ï¼ˆå¸¶é‡è©¦æ©Ÿåˆ¶å’Œç·©å­˜ï¼‰
        
        Args:
            user_message: ç”¨æˆ¶è¨Šæ¯
            conversation_context: å°è©±ä¸Šä¸‹æ–‡ï¼ˆå¯é¸ï¼‰
            language: èªè¨€ä»£ç¢¼ ('zh' æˆ– 'en')
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ˆç”¨æ–¼è™•ç†é…é¡é™åˆ¶ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç·©å­˜ï¼ˆé»˜èª Trueï¼‰
            
        Returns:
            str: Gemini ç”Ÿæˆçš„å›æ‡‰ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None
        """
        if not self.is_available():
            logger.warning("Gemini API ä¸å¯ç”¨ï¼Œç„¡æ³•ç”Ÿæˆå›æ‡‰")
            return None
        
        # é©—è­‰è¼¸å…¥
        if not user_message or not user_message.strip():
            logger.warning("ç”¨æˆ¶è¨Šæ¯ç‚ºç©ºï¼Œç„¡æ³•ç”Ÿæˆå›æ‡‰")
            return None
        
        # æ¸…ç†å’Œé™åˆ¶è¼¸å…¥é•·åº¦
        user_message = user_message.strip()
        if len(user_message) > 500:  # é™åˆ¶è¼¸å…¥é•·åº¦
            user_message = user_message[:500]
            logger.warning("ç”¨æˆ¶è¨Šæ¯éé•·ï¼Œå·²æˆªæ–·è‡³ 500 å­—ç¬¦")
        
        # æª¢æŸ¥ç·©å­˜ï¼ˆåƒ…å°ç°¡å–®å•é¡Œä½¿ç”¨ç·©å­˜ï¼Œä¸åŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        if use_cache and not conversation_context:
            cached_response = self.cache.get(user_message, language)
            if cached_response:
                self.stats['cache_hits'] += 1
                logger.debug(f"å¾ç·©å­˜ç²å–å›æ‡‰ï¼ˆèªè¨€: {language}ï¼‰")
                return cached_response
        
        self.stats['cache_misses'] += 1
        self.stats['total_requests'] += 1
        
        for attempt in range(max_retries + 1):
            try:
                # æ§‹å»ºç³»çµ±æç¤ºè©
                system_prompt = self._build_system_prompt(language)
                
                # æ§‹å»ºå®Œæ•´æç¤ºè©
                full_prompt = self._build_prompt(
                    system_prompt,
                    user_message,
                    conversation_context,
                    language
                )
                
                # ç²å–æ¨¡å‹
                model = genai.GenerativeModel(self.model_name)
                
                # æ ¹æ“šèªè¨€å’Œå•é¡Œé¡å‹å„ªåŒ–ç”Ÿæˆé…ç½®
                generation_config = self._get_optimized_generation_config(language, user_message)
                
                # ç”Ÿæˆå›æ‡‰
                response = model.generate_content(
                    full_prompt,
                    generation_config=generation_config
                )
                
                if response and response.text:
                    response_text = response.text.strip()
                    
                    # é©—è­‰å’Œæ¸…ç†å›æ‡‰
                    response_text = self._validate_and_clean_response(response_text, language)
                    
                    if response_text:
                        # ä¿å­˜åˆ°ç·©å­˜ï¼ˆåƒ…å°ç°¡å–®å•é¡Œï¼‰
                        if use_cache and not conversation_context:
                            self.cache.set(user_message, language, response_text)
                        
                        self.stats['successful_responses'] += 1
                        logger.info(f"Gemini API å›æ‡‰ç”ŸæˆæˆåŠŸï¼ˆé•·åº¦: {len(response_text)} å­—ç¬¦ï¼‰")
                        return response_text
                    else:
                        logger.warning("Gemini API å›æ‡‰é©—è­‰å¤±æ•—")
                        return None
                else:
                    logger.warning("Gemini API è¿”å›ç©ºå›æ‡‰")
                    return None
                    
            except Exception as e:
                error_msg = str(e)
                
                # è™•ç†é…é¡é™åˆ¶ï¼ˆ429 éŒ¯èª¤ï¼‰
                if "429" in error_msg or "quota" in error_msg.lower() or "Quota exceeded" in error_msg:
                    self.stats['api_errors'] += 1
                    if attempt < max_retries:
                        # å˜—è©¦å¾éŒ¯èª¤è¨Šæ¯ä¸­æå–é‡è©¦å»¶é²æ™‚é–“
                        retry_delay = self._extract_retry_delay(error_msg)
                        logger.warning(
                            f"Gemini API é…é¡é™åˆ¶ï¼Œç­‰å¾… {retry_delay} ç§’å¾Œé‡è©¦ "
                            f"({attempt + 1}/{max_retries + 1})"
                        )
                        time.sleep(retry_delay)
                        continue
                    else:
                        logger.error("Gemini API é…é¡é™åˆ¶ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                        return None
                
                # è™•ç†èªè­‰éŒ¯èª¤ï¼ˆ401/403ï¼‰- ä¸é‡è©¦
                if "401" in error_msg or "403" in error_msg or "API_KEY_INVALID" in error_msg:
                    logger.error("Gemini API èªè­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥ API key æ˜¯å¦æœ‰æ•ˆ")
                    return None
                
                # è™•ç†å…¶ä»–éŒ¯èª¤
                # ç§»é™¤å¯èƒ½çš„ API key æ´©éœ²
                if self.api_key and self.api_key in error_msg:
                    error_msg = error_msg.replace(self.api_key, '[REDACTED]')
                
                logger.error(f"Gemini API èª¿ç”¨å¤±æ•—: {error_msg}")
                
                # å°æ–¼éé…é¡éŒ¯èª¤ï¼Œä¸é‡è©¦ï¼Œç›´æ¥è¿”å›
                # é…é¡éŒ¯èª¤å·²ç¶“åœ¨ä¸Šé¢è™•ç†äº†ï¼ˆæœƒ continue é‡è©¦ï¼‰
                return None
        
        return None
    
    def _extract_retry_delay(self, error_msg: str) -> float:
        """
        å¾éŒ¯èª¤è¨Šæ¯ä¸­æå–é‡è©¦å»¶é²æ™‚é–“
        
        Args:
            error_msg: éŒ¯èª¤è¨Šæ¯
            
        Returns:
            float: é‡è©¦å»¶é²æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œé»˜èª 16 ç§’
        """
        try:
            # å˜—è©¦å¾éŒ¯èª¤è¨Šæ¯ä¸­æå– retry_delay
            if "retry_delay" in error_msg.lower():
                import re
                # æŸ¥æ‰¾é¡ä¼¼ "seconds: 15" çš„æ¨¡å¼
                match = re.search(r'seconds[:\s]+(\d+(?:\.\d+)?)', error_msg)
                if match:
                    return float(match.group(1)) + 1.0  # åŠ  1 ç§’ç·©è¡
        except Exception:
            pass
        
        # é»˜èªå»¶é²æ™‚é–“
        return 16.0
    
    def _build_system_prompt(self, language: str) -> str:
        """
        æ§‹å»ºå„ªåŒ–çš„ç³»çµ±æç¤ºè©ï¼ˆåŒ…å« few-shot examplesï¼‰
        
        Args:
            language: èªè¨€ä»£ç¢¼
            
        Returns:
            str: ç³»çµ±æç¤ºè©
        """
        if language == 'en':
            return """You are a helpful and friendly campus assistant chatbot for National Formosa University (NFU).

Your primary role is to help students and visitors:
- Find campus facilities (restrooms, water fountains, trash cans)
- Answer questions about campus information
- Provide navigation and directions
- Assist with general campus inquiries

Guidelines:
- Keep responses concise, friendly, and helpful (under 200 words)
- If asked about facilities, suggest using the map feature
- If you don't know specific information, politely redirect to relevant resources
- Maintain a warm, professional tone
- Use emojis sparingly and appropriately

Example good responses:
User: "Where is the nearest restroom?"
You: "I can help you find the nearest restroom! Please use the map feature on the right side, or tell me your current location and I'll guide you there. ğŸš»"

User: "What's the weather today?"
You: "I don't have real-time weather information, but I recommend checking a weather app or website for the latest forecast. Is there anything else about campus facilities I can help with? ğŸŒ¤ï¸"

Remember: Be helpful, concise, and always try to guide users to useful resources."""
        else:
            return """ä½ æ˜¯ä¸€å€‹å‹å–„ä¸”å°ˆæ¥­çš„æ ¡åœ’åŠ©æ‰‹èŠå¤©æ©Ÿå™¨äººï¼Œæœå‹™æ–¼åœ‹ç«‹è™å°¾ç§‘æŠ€å¤§å­¸ï¼ˆNFUï¼‰ã€‚

ä½ çš„ä¸»è¦è·è²¬æ˜¯å¹«åŠ©å­¸ç”Ÿå’Œè¨ªå®¢ï¼š
- æŸ¥æ‰¾æ ¡åœ’è¨­æ–½ï¼ˆå»æ‰€ã€é£²æ°´æ©Ÿã€åƒåœ¾æ¡¶ï¼‰
- å›ç­”æ ¡åœ’ç›¸é—œå•é¡Œ
- æä¾›å°èˆªå’Œè·¯ç·šæŒ‡å¼•
- å”åŠ©ä¸€èˆ¬æ ¡åœ’æŸ¥è©¢

å›æ‡‰æŒ‡å—ï¼š
- ä¿æŒå›æ‡‰ç°¡æ½”ã€å‹å–„ä¸”æœ‰ç”¨ï¼ˆ200 å­—ä»¥å…§ï¼‰
- å¦‚æœè©¢å•è¨­æ–½ï¼Œå»ºè­°ä½¿ç”¨å³å´åœ°åœ–åŠŸèƒ½
- å¦‚æœä¸çŸ¥é“å…·é«”è³‡è¨Šï¼Œç¦®è²Œåœ°å¼•å°åˆ°ç›¸é—œè³‡æº
- ä¿æŒæº«æš–ã€å°ˆæ¥­çš„èªèª¿
- é©åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿ

è‰¯å¥½å›æ‡‰ç¯„ä¾‹ï¼š
ç”¨æˆ¶ï¼šã€Œæœ€è¿‘çš„å»æ‰€åœ¨å“ªè£¡ï¼Ÿã€
ä½ ï¼šã€Œæˆ‘å¯ä»¥å¹«ä½ æ‰¾æœ€è¿‘çš„å»æ‰€ï¼è«‹ä½¿ç”¨å³å´çš„åœ°åœ–åŠŸèƒ½ï¼Œæˆ–å‘Šè¨´æˆ‘ä½ ç›®å‰çš„ä½ç½®ï¼Œæˆ‘æœƒç‚ºä½ æŒ‡å¼•ã€‚ğŸš»ã€

ç”¨æˆ¶ï¼šã€Œä»Šå¤©å¤©æ°£å¦‚ä½•ï¼Ÿã€
ä½ ï¼šã€Œæˆ‘æ²’æœ‰å³æ™‚å¤©æ°£è³‡è¨Šï¼Œå»ºè­°ä½ æŸ¥çœ‹å¤©æ°£é å ± App æˆ–ç¶²ç«™ã€‚é‚„æœ‰å…¶ä»–é—œæ–¼æ ¡åœ’è¨­æ–½çš„å•é¡Œæˆ‘å¯ä»¥å”åŠ©å—ï¼ŸğŸŒ¤ï¸ã€

è¨˜ä½ï¼šè¦å‹å–„ã€ç°¡æ½”ï¼Œä¸¦å§‹çµ‚å¼•å°ç”¨æˆ¶ä½¿ç”¨æœ‰ç”¨çš„è³‡æºã€‚"""
    
    def _build_prompt(
        self,
        system_prompt: str,
        user_message: str,
        conversation_context: Optional[list],
        language: str
    ) -> str:
        """
        æ§‹å»ºå„ªåŒ–çš„å®Œæ•´æç¤ºè©ï¼ˆæ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†ï¼‰
        
        Args:
            system_prompt: ç³»çµ±æç¤ºè©
            user_message: ç”¨æˆ¶è¨Šæ¯
            conversation_context: å°è©±ä¸Šä¸‹æ–‡
            language: èªè¨€ä»£ç¢¼
            
        Returns:
            str: å®Œæ•´æç¤ºè©
        """
        prompt_parts = [system_prompt]
        
        # æ™ºèƒ½æ·»åŠ å°è©±ä¸Šä¸‹æ–‡ï¼ˆå„ªåŒ–ç‰ˆæœ¬ï¼‰
        if conversation_context:
            # éæ¿¾å’Œå£“ç¸®ä¸Šä¸‹æ–‡
            filtered_context = self._filter_and_compress_context(conversation_context, language)
            
            if filtered_context:
                if language == 'en':
                    prompt_parts.append("\n\nRecent conversation context:")
                else:
                    prompt_parts.append("\n\næœ€è¿‘çš„å°è©±ä¸Šä¸‹æ–‡ï¼š")
                
                for ctx in filtered_context:
                    prompt_parts.append(f"- {ctx}")
        
        # æ·»åŠ ç”¨æˆ¶è¨Šæ¯ï¼ˆå„ªåŒ–æ ¼å¼ï¼‰
        if language == 'en':
            prompt_parts.append(f"\n\nUser question: {user_message}")
            prompt_parts.append("\n\nPlease provide a concise and helpful response:")
        else:
            prompt_parts.append(f"\n\nç”¨æˆ¶å•é¡Œï¼š{user_message}")
            prompt_parts.append("\n\nè«‹æä¾›ç°¡æ½”ä¸”æœ‰ç”¨çš„å›æ‡‰ï¼š")
        
        return "\n".join(prompt_parts)
    
    def _filter_and_compress_context(
        self,
        conversation_context: List[str],
        language: str
    ) -> List[str]:
        """
        éæ¿¾å’Œå£“ç¸®å°è©±ä¸Šä¸‹æ–‡ï¼Œåªä¿ç•™ç›¸é—œå…§å®¹
        
        Args:
            conversation_context: åŸå§‹ä¸Šä¸‹æ–‡åˆ—è¡¨
            language: èªè¨€ä»£ç¢¼
            
        Returns:
            List[str]: éæ¿¾å¾Œçš„ä¸Šä¸‹æ–‡åˆ—è¡¨
        """
        if not conversation_context:
            return []
        
        filtered = []
        max_context_items = 3  # æ¸›å°‘åˆ° 3 æ¢ä»¥ç¯€çœ token
        
        # åªå–æœ€è¿‘çš„å¹¾æ¢ï¼Œä¸¦å£“ç¸®é•·åº¦
        for ctx in conversation_context[-max_context_items:]:
            # å£“ç¸®éé•·çš„ä¸Šä¸‹æ–‡
            if len(ctx) > 150:
                ctx = ctx[:150] + "..."
            filtered.append(ctx)
        
        return filtered
    
    def _get_optimized_generation_config(self, language: str, user_message: str) -> Dict[str, Any]:
        """
        æ ¹æ“šèªè¨€å’Œå•é¡Œé¡å‹å„ªåŒ–ç”Ÿæˆé…ç½®
        
        Args:
            language: èªè¨€ä»£ç¢¼
            user_message: ç”¨æˆ¶è¨Šæ¯
            
        Returns:
            Dict: å„ªåŒ–çš„ç”Ÿæˆé…ç½®
        """
        # æª¢æ¸¬å•é¡Œé¡å‹
        is_simple_question = len(user_message) < 50
        is_complex_question = len(user_message) > 200 or '?' in user_message or 'ï¼Ÿ' in user_message
        
        # åŸºç¤é…ç½®
        config = {
            'temperature': 0.7,  # å¹³è¡¡å‰µé€ æ€§å’Œæº–ç¢ºæ€§
            'top_p': 0.8,  # æ ¸æ¡æ¨£
            'top_k': 40,  # Top-K æ¡æ¨£
            'max_output_tokens': 512,  # æ¸›å°‘ token ä½¿ç”¨ï¼ˆå¾ 1024 é™åˆ° 512ï¼‰
        }
        
        # æ ¹æ“šå•é¡Œé¡å‹èª¿æ•´
        if is_simple_question:
            # ç°¡å–®å•é¡Œï¼šæ›´ç¢ºå®šæ€§ï¼Œæ›´çŸ­å›æ‡‰
            config['temperature'] = 0.5
            config['max_output_tokens'] = 256
        elif is_complex_question:
            # è¤‡é›œå•é¡Œï¼šå…è¨±æ›´å¤šå‰µé€ æ€§ï¼Œæ›´é•·å›æ‡‰
            config['temperature'] = 0.8
            config['max_output_tokens'] = 512
        
        return config
    
    def _validate_and_clean_response(self, response: str, language: str) -> Optional[str]:
        """
        é©—è­‰å’Œæ¸…ç† API å›æ‡‰
        
        Args:
            response: API å›æ‡‰
            language: èªè¨€ä»£ç¢¼
            
        Returns:
            æ¸…ç†å¾Œçš„å›æ‡‰ï¼Œå¦‚æœç„¡æ•ˆå‰‡è¿”å› None
        """
        if not response or not response.strip():
            return None
        
        # æ¸…ç†å›æ‡‰
        response = response.strip()
        
        # ç§»é™¤éé•·çš„å›æ‡‰ï¼ˆè¶…é 1000 å­—ç¬¦ï¼‰
        if len(response) > 1000:
            logger.warning(f"å›æ‡‰éé•·ï¼ˆ{len(response)} å­—ç¬¦ï¼‰ï¼Œå·²æˆªæ–·")
            response = response[:1000] + "..."
        
        # ç§»é™¤å¯èƒ½çš„é‡è¤‡å…§å®¹
        lines = response.split('\n')
        seen = set()
        cleaned_lines = []
        for line in lines:
            line_stripped = line.strip()
            if line_stripped and line_stripped not in seen:
                seen.add(line_stripped)
                cleaned_lines.append(line)
        
        response = '\n'.join(cleaned_lines)
        
        # é©—è­‰å›æ‡‰åŒ…å«å¯¦éš›å…§å®¹ï¼ˆä¸åªæ˜¯æ¨™é»ç¬¦è™Ÿï¼‰
        if len(response.replace(' ', '').replace('\n', '').replace('\t', '')) < 5:
            logger.warning("å›æ‡‰å…§å®¹éå°‘ï¼Œå¯èƒ½ç„¡æ•ˆ")
            return None
        
        return response
    
    def get_stats(self) -> Dict[str, Any]:
        """
        ç²å–çµ±è¨ˆè³‡è¨Š
        
        Returns:
            Dict: çµ±è¨ˆè³‡è¨Š
        """
        cache_hit_rate = 0.0
        if self.stats['total_requests'] > 0:
            cache_hit_rate = self.stats['cache_hits'] / (
                self.stats['cache_hits'] + self.stats['cache_misses']
            ) * 100
        
        return {
            **self.stats,
            'cache_hit_rate': f"{cache_hit_rate:.1f}%",
            'cache_size': len(self.cache.cache),
            'model': self.model_name
        }
    
    def clear_cache(self) -> None:
        """æ¸…ç©ºç·©å­˜"""
        self.cache.clear()
        logger.info("Gemini éŸ¿æ‡‰ç·©å­˜å·²æ¸…ç©º")


# å…¨å±€å®¢æˆ¶ç«¯å¯¦ä¾‹
_gemini_client: Optional[GeminiClient] = None


def get_gemini_client() -> GeminiClient:
    """
    ç²å– Gemini å®¢æˆ¶ç«¯å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        GeminiClient: Gemini å®¢æˆ¶ç«¯å¯¦ä¾‹
    """
    global _gemini_client
    if _gemini_client is None:
        _gemini_client = GeminiClient()
    return _gemini_client

